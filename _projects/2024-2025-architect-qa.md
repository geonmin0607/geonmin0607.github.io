---
title: "건축민원 AI 답변생성 서비스"
subtitle: "LLM-as-a-Judge 평가 · 질문 자동 분리 프롬프트"
date: 2025-02-01
show_date: false
categories: [Projects]
tags: [Prompt Tuning, LLM-as-a-Judge, Domain QA]
excerpt: |
  <ul class="excerpt-list">
    <li><strong>기간:</strong> 2024.09.01 ~ 2025.02.01</li>
    <li>건축 민원 자동응답용 QA 모델</li>
    <li>Prompt Tuning · LLM-as-a-Judge · Domain QA</li>
  </ul>
---

## 개요

실제 **건축 민원 문장**을 분석해  
질문을 자동 분리하고, 규정·지침을 근거로 **답변을 생성·검증**하는  
LLM 기반 민원 응답 시스템을 구축한 프로젝트입니다.

## 진행기간
2024.09.01 ~ 2025.02.01

## 배경 / 문제

- 한 문장 안에 여러 질문·요청이 섞여 있는 민원이 많음
- 답변 품질을 객관적으로 평가할 수 있는 기준·지표가 부족함
- 도메인 전문가의 피드백을 모델 학습에 잘 반영하기 어려움

## 내가 한 일

- 실제 민원 데이터를 분석해 **질문 자동 분리 프롬프트** 설계
  - 한 민원에서 “질문 단위”를 어떻게 나눌지 규칙 정의
- 답변 품질을 평가하기 위한 **오류 유형·루브릭·샘플** 정의
  - 규정 오인용, 조건 누락, 범위 오해 등 세부 오류 타입 설계
- LLM-as-a-Judge를 활용한 **자동 평가 파이프라인** 구성
  - 기준답변·루브릭·샘플을 이용한 점수 계산 구조 설계
- 평가 결과를 다시 학습에 반영하는  
  **Close-the-Loop 실험** (데이터 선택 → 재학습 → 재평가)

## 결과 / 성과

- 민원 문장을 **질문 단위로 구조화**하는 프롬프트/프로세스 정립
- 도메인 전문가의 판단을 **오류 유형·점수 체계**로 옮겨  
  자동 평가에 활용할 수 있게 함
- 평가–재학습 루프를 통해 답변 정확도·일관성이 개선되는 방향 확인

## 사용 기술

- LLM (프롬프트 엔지니어링, LoRA/FT)
- LLM-as-a-Judge
- 평가 루브릭·샘플 설계
- 데이터 파이프라인 (평가 결과 → 재학습 데이터 구성)
